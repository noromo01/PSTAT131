---
title: "Homework_2"
author: "Noah Moyer"
date: "2023-01-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Read in required packages
```{r}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(kableExtra)
library(here)
library(kknn)
tidymodels_prefer()
```

### Read in the dataset
```{r}

abalone <- read.csv(here("Homework", "Homework2", "data", "abalone.csv"))

```

## Question 1
```{r}
#create a new column called age based on the number of rings plus 1.5
abalone$age <- abalone$rings + 1.5

#make a histogram to describe the distribution of the age variable
ggplot(abalone, aes(x=age)) +
  geom_histogram(color="black", fill="grey") +
  labs(title="Age of Abalone Animal", x="Age", y="Count")

#make a boxplot
ggplot(abalone, aes(x=age)) +
  geom_boxplot() +
  labs(title="Age of Abalone Animal", x="Age")
  

```

Thus, the distribution of the age of abalones is approximately normal with a fairly strong right skew. A boxplot reveals that there are a number of outliers on both ends of the boxplot but most abalones are concentrated around the 9-14 age.

## Question 2
```{r}
#set seed so we get reproducible results
set.seed(1246)

#split the data set into a training and a testing set
abaloneSplit <- initial_split(abalone, prop=0.8, strata=age)

#training set
abaloneTraining <- training(abaloneSplit)

#testing set
abaloneTesting <- testing(abaloneSplit)
```

## Question 3
```{r}
#create a recipe with all predictors
abaloneRecipe <- recipe(age ~ . , data=abaloneTraining)

#remove rings predictor and add some interaction terms
abaloneRecipeFinal <- abaloneRecipe %>% 
    step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms=~shucked_weight:starts_with("type")+longest_shell:diameter+shucked_weight:shell_weight)  %>% 
  step_rm(rings) %>%
  step_center() %>%
  step_scale()
```

We must remove rings because if rings is included the algorithm will just use rings to predict age since age has direct linear correlation to the number of rings.

### Question 4
```{r}
#bake the data
prep(abaloneRecipeFinal) %>% 
  bake(new_data=abaloneTraining) %>% 
  head() %>% 
  kable() %>% 
  kable_styling(full_width=FALSE) %>% 
  scroll_box(width="100%", height="200px")

#create an linear model
lmAbalone <- linear_reg() %>% 
  set_engine("lm")
```

### Question 5
```{r}
#create a k nearest neighbors model
knnAbalone <- nearest_neighbor(neighbors=7) %>% 
  set_engine("kknn") %>% 
  set_mode("regression")
```

### Question 6
```{r}
#create a workflow for linear model
lmWorkflowAbalone <- workflow() %>% 
  add_model(lmAbalone)  %>% 
  add_recipe(abaloneRecipeFinal)

#create a workflow for the knn model
knnWorkflowAbalone <- workflow() %>% 
  add_model(knnAbalone) %>% 
  add_recipe(abaloneRecipeFinal)

#fit lm model to the training set
lmFitAbalone <- fit(lmWorkflowAbalone, abaloneTraining)

#fit knn model to the training set
knnFitAbalone <- fit(knnWorkflowAbalone, abaloneTraining)
```


## Question 7
```{r}
#insert the new data, note that rings is required but does not influence the results
newDataTest <- data.frame(type=c("F"),  longest_shell=c(0.5),  diameter=c(0.1),  height=c(.3),   whole_weight=c(4), shucked_weight=c(1),  viscera_weight=c(2),  shell_weight=c(1), rings=c(3))

#return the predicted value of newDataTest
predict(lmFitAbalone, new_data=newDataTest)

```

## Question 8
```{r}
library(yardstick)

#create a metric set
testingMetrics <- metric_set(rsq, rmse, mae)

#apply the testing data to the algorithm
testingOutcomeLm <- predict(lmFitAbalone, new_data=abaloneTesting %>% select(-age))
testingOutcomeKNN <- predict(knnFitAbalone, new_data=abaloneTesting %>% select(-age))

#attach it to our testing data
abaloneTestingResLm <- bind_cols(testingOutcomeLm, abaloneTesting %>% select(age))
abaloneTestingResKNN <- bind_cols(testingOutcomeKNN, abaloneTesting %>% select(age))

#apply the testing matrix
testingMetrics(abaloneTestingResLm, truth=age, estimate=.pred)
testingMetrics(abaloneTestingResKNN, truth=age, estimate=.pred)
```

The r-squared value for the linear model is fairly low. 0.56 shows some correlation but not a whole lot. So the model is making some positive progress but is not entirely impressive. The r-squared value for the K-nearest neighbors is slightly lower. This means that K-nearest neighbors is a slightly worse model.

## Question 9
The linear model performed better. This is because I think we may have overfit the data with the KNN model. The KNN value was fairly high which can lead to overfitting. I am surprised by how low the r-squared value is for both datasets. I'd think it would be fairly easy to predict age based on factors like weight but apparently this is not so. Perhaps even the linear model is too simple of a model.