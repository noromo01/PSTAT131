---
title: "Homework5_solutions"
author: "Noah Moyer"
date: "2023-03-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Getting set up
```{r}
#load in packages
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(janitor)
library(naniar)
library(xgboost)
library(ranger)
library(vip)
library(corrplot)
library(here)
library(ggplot2)
library(forcats)
tidymodels_prefer()

#read in data
pokemon <- read.csv(here("Homework", "Homework5", "data", "Pokemon.csv"))
```

### Exercise 1
```{r}
#convert to snake case
pokemon <- as_tibble(pokemon) %>% 
  clean_names()
```

It changed all the variable names so that instead of many of the names including periods all words are simply seperated by an underscore. This is useful because it means that variable names are easily predictable and it was a lot more efficient than trying to rename the columns one at a time.

### Exercise 2
```{r}
#create the bar chart
ggplot(data=pokemon, aes(x=type_1)) +
  geom_bar(stat="count") +
  theme(axis.text.x=element_text(angle=45))
```

There are 18 classes. The flying class has an especially low count but fairy also has a somewhat lower count.  
I will now convert the low count classes into an "other" class.
```{r}
#make type a factor and lump together the top 6, the rest are classified as other
pokemon$type_1 <- factor(pokemon$type_1) %>% 
  fct_lump_n(6)

#make legendary a factor
pokemon$legendary <- factor(pokemon$legendary)

#make generation a factor
pokemon$generation <- factor(pokemon$generation)
```

### Exercise 3
```{r}
set.seed(2506)

#split the data
poke_split <- initial_split(pokemon, prop=0.75, strata="type_1")

#classify as training or testing
poke_train <- training(poke_split)
poke_test <- training(poke_split)

#set up k-fold cross validation
poke_folds <- vfold_cv(poke_train, v=5, strata="type_1")
```

It is useful to stratify by type in v-fold so we don't have folds that have an overrepresentation of a certain type.

### Exercise 4
```{r}
#creating a correlation plot of only numeric continuous variables
pokemon %>% 
  select(is.numeric, -x, -generation) %>% 
  cor() %>% 
  corrplot(type="lower", diag=FALSE)
```
It appears that total has a high correlation with the rest of the predictors. This is probably because total is a sum of the other components. I do not think we need to include total in this analysis. Defense and sp_def, sp_atk and sp_def, and speed and sp_atk all have fairly high correlation but it is not high enough for me to exclude them from our analysis. Otherwise, all of the other variables have a fairly low correlation. Something interesting to note is the fact that none of the variables have a negative correlation, they all have positive correlations. It seems like somethign like attack and defense would have a negative correlation because maybe as you get better at attacking your defenses get worse and vica versa. This does not appear to be the case. Generally, as attack increases so does defense.

### Exercise 5
```{r}
#create the recipe
rec_poke <- recipe(type_1~legendary+generation+sp_atk+attack+speed+defense+hp+sp_def, data=poke_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```

### Exercise 6
```{r}
#set up model for elastic net multinominal regression (ENMR)
ENMR <- multinom_reg(mixture=tune(), penalty=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

#set up workflow for ENMR
ENMR_wf_poke <- workflow() %>% 
  add_model(ENMR) %>% 
  add_recipe(rec_poke)

#set up grid for ENMR
ENMR_grid <- grid_regular(penalty(range=c(0.01,3)), 
                          mixture(range=c(0,1)), 
                          levels=10)
```

### Exercise 7
```{r}
#set up model for random forest
random_forest <- rand_forest(mtry=tune(), trees=tune(), min_n=tune()) %>% 
  set_engine("ranger", importance="impurity") %>% 
  set_mode("classification")

#set up workflow for random forest
random_forest_wf_poke <- workflow() %>% 
  add_model(random_forest) %>% 
  add_recipe(rec_poke)

#set up grid for random forest
random_forest_grid <- grid_regular(mtry(range=c(1,8)),
                                   trees(range=c(100,800)),
                                   min_n(range=c(10,18)),
                                   levels=8)
```

Mtry is how many predictors the algorithm will randomly select to make each split. After a random set of predictors is selected, the algorithm will then choose the best predictor to split the data on. This is in order to reduce dependence with trees and minimize pairwise correlation.  
Trees is how many decision trees there are in the forest. The result from each tree will be averaged to make a final prediction.  
Min_n is how many results need to be at a node of the training dataset for the tree to stop branching and place a leaf/node instead.  
Mtry being less than 1 or greater than 8 would not make sense. Greater than 8 would not make sense because we only have 8 predictors. Less than 1 would not make sense because it would be impossible to select 0 or -1 predictors.  
Mtry=8 represents a bagging linear model.  

#Exercise 8
```{r, eval=FALSE}
#fit the ENMR model
ENMR_fit <- tune_grid(
  object=ENMR_wf_poke,
  resamples=poke_folds,
  grid=ENMR_grid,
  control=control_grid(verbose=TRUE)
)

#fit the random forest model
random_forest_fit <- tune_grid(
  object=random_forest_wf_poke,
  resamples=poke_folds,
  grid=random_forest_grid,
  control=control_grid(verbose=TRUE)
)

#save the runs
save(ENMR_fit, file=here("Homework", "Homework5", "rdas", "ENMR_fit.rda"))
save(random_forest_fit, file=here("Homework", "Homework5", "rdas", "ENMR_fit.rda"))

```
