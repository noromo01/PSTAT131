---
title: "Homework5_solutions"
author: "Noah Moyer"
date: "2023-03-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Note that if you want to be able to run this code without running the whole models, you will need to modify the file paths on line 175 and 176. The code is currently set up for my personal file path inside my project.

### Getting set up
```{r}
#load in packages
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(janitor)
library(naniar)
library(xgboost)
library(ranger)
library(vip)
library(corrplot)
library(here)
library(ggplot2)
library(forcats)
tidymodels_prefer()

#read in data
pokemon <- read.csv(here("Homework", "Homework5", "data", "Pokemon.csv"))
```

### Exercise 1
```{r}
#convert to snake case
pokemon <- as_tibble(pokemon) %>% 
  clean_names()
```

It changed all the variable names so that instead of many of the names including periods all words are simply seperated by an underscore. This is useful because it means that variable names are easily predictable and it was a lot more efficient than trying to rename the columns one at a time.

### Exercise 2
```{r}
#create the bar chart
ggplot(data=pokemon, aes(x=type_1)) +
  geom_bar(stat="count") +
  theme(axis.text.x=element_text(angle=45))
```

There are 18 classes. The flying class has an especially low count but fairy also has a somewhat lower count.  
I will now convert the low count classes into an "other" class.
```{r}
#make type a factor and lump together the top 6, the rest are classified as other
pokemon$type_1 <- factor(pokemon$type_1) %>% 
  fct_lump_n(6)

#make legendary a factor
pokemon$legendary <- factor(pokemon$legendary)

#make generation a factor
pokemon$generation <- factor(pokemon$generation)
```

### Exercise 3
```{r}
set.seed(2506)

#split the data
poke_split <- initial_split(pokemon, prop=0.75, strata="type_1")

#classify as training or testing
poke_train <- training(poke_split)
poke_test <- training(poke_split)

#set up k-fold cross validation
poke_folds <- vfold_cv(poke_train, v=5, strata="type_1")
```

It is useful to stratify by type in v-fold so we don't have folds that have an overrepresentation of a certain type.

### Exercise 4
```{r}
#creating a correlation plot of only numeric continuous variables
pokemon %>% 
  select(is.numeric, -x, -generation) %>% 
  cor() %>% 
  corrplot(type="lower", diag=FALSE)
```

It appears that total has a high correlation with the rest of the predictors. This is probably because total is a sum of the other components. I do not think we need to include total in this analysis. Defense and sp_def, sp_atk and sp_def, and speed and sp_atk all have fairly high correlation but it is not high enough for me to exclude them from our analysis. Otherwise, all of the other variables have a fairly low correlation. Something interesting to note is the fact that none of the variables have a negative correlation, they all have positive correlations. It seems like somethign like attack and defense would have a negative correlation because maybe as you get better at attacking your defenses get worse and vica versa. This does not appear to be the case. Generally, as attack increases so does defense.

### Exercise 5
```{r}
#create the recipe
rec_poke <- recipe(type_1~legendary+generation+sp_atk+attack+speed+defense+hp+sp_def, data=poke_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```

### Exercise 6
```{r}
#set up model for elastic net multinominal regression (ENMR)
ENMR <- multinom_reg(mixture=tune(), penalty=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

#set up workflow for ENMR
ENMR_wf_poke <- workflow() %>% 
  add_model(ENMR) %>% 
  add_recipe(rec_poke)

#set up grid for ENMR
ENMR_grid <- grid_regular(penalty(range=c(0.01,3)), 
                          mixture(range=c(0,1)), 
                          levels=10)
```

### Exercise 7
```{r}
#set up model for random forest
random_forest <- rand_forest(mtry=tune(), trees=tune(), min_n=tune()) %>% 
  set_engine("ranger", importance="impurity") %>% 
  set_mode("classification")

#set up workflow for random forest
random_forest_wf_poke <- workflow() %>% 
  add_model(random_forest) %>% 
  add_recipe(rec_poke)

#set up grid for random forest
random_forest_grid <- grid_regular(mtry(range=c(1,8)),
                                   trees(range=c(100,800)),
                                   min_n(range=c(10,18)),
                                   levels=8)
```

Mtry is how many predictors the algorithm will randomly select to make each split. After a random set of predictors is selected, the algorithm will then choose the best predictor to split the data on. This is in order to reduce dependence with trees and minimize pairwise correlation.  
Trees is how many decision trees there are in the forest. The result from each tree will be averaged to make a final prediction.  
Min_n is how many results need to be at a node of the training dataset for the tree to stop branching and place a leaf/node instead.  
Mtry being less than 1 or greater than 8 would not make sense. Greater than 8 would not make sense because we only have 8 predictors. Less than 1 would not make sense because it would be impossible to select 0 or -1 predictors.  
Mtry=8 represents a bagging linear model.  

#Exercise 8
```{r, eval=FALSE}
#fit the ENMR model
ENMR_fit <- tune_grid(
  object=ENMR_wf_poke,
  resamples=poke_folds,
  grid=ENMR_grid,
  control=control_grid(verbose=TRUE)
)

#fit the random forest model
random_forest_fit <- tune_grid(
  object=random_forest_wf_poke,
  resamples=poke_folds,
  grid=random_forest_grid,
  control=control_grid(verbose=TRUE)
)

#save the runs
save(ENMR_fit, file=here("Homework", "Homework5", "rdas", "ENMR_fit.rda"))
save(random_forest_fit, file=here("Homework", "Homework5", "rdas", "random_forest_fit.rda"))

```

```{r}
#read back in the runs
load(here("Homework", "Homework5", "rdas", "ENMR_fit.rda"))
load(here("Homework", "Homework5", "rdas", "random_forest_fit.rda"))
```

I will now investigate how the models did.
```{r}
#check out the ENMR model
autoplot(ENMR_fit) + theme_minimal()

#check out the random forest model
autoplot(random_forest_fit) + theme_minimal()
```

Both of these plots are somewhat overwhelming to me. I am going to guess which models performed the best based on the graphs and then use show_best to return the best model. It appears that low levels of penalty and regularization return better results. For the random forest it appears that higher numbers of randomly selected predictors performed better. It doesn't appear as if there is a huge amount of difference across node size and also not a huge amount of difference across the number of trees. We should note, however, that more trees seems to perform slightly better.

I will now return the definitive best model for both fits.
```{r}
#return the best 5 models for ENMR
show_best(ENMR_fit, n=5)

#return the best 5 models for random_forest
show_best(random_forest_fit, n=5)
```

The best elastic net model had a penalty of 1.023293 and no mixture. The best random forest has 3 randomly selected predictors, only 100 trees, and a minimum number of nodes of 13.

### Exercise 9
```{r}
#select the best random forest model
best_random_forest <- select_best(random_forest_fit)

#fit to training set
final_random_forest_poke <- finalize_workflow(random_forest_wf_poke, best_random_forest) %>% 
  fit(poke_train)

#evaluate on the testing set
poke_random_forest_test <- augment(final_random_forest_poke,
                                   poke_test) %>% 
  select(type_1, starts_with(".pred"))

#let's see how well the model worked
roc_auc(poke_random_forest_test, truth=type_1, .pred_Bug:.pred_Other)
```

It appears that the model has worked extremely well.

```{r}
#variable importance plot
final_random_forest_poke %>% extract_fit_parsnip() %>% 
  vip() +
  theme_minimal()
```

The sp_atk variable appears to be the most important. The attack, speed, defense, hp, and sp_def variables are also important, and almost equally important. Notably, the generation that the pokemon is from was not important. I am surprised how all of the variables that build the pokemon are all of relatively equal importance. I am not surprised that generation was not important. When a new generation is created, it would be weird if there was a lot more of one type of pokemon compared to another.

```{r}
#create roc curves of the different types of pokemon
roc_curve(poke_random_forest_test, truth=type_1, .pred_Bug:.pred_Other) %>% 
  autoplot()

#create a confusion matrix
conf_mat(poke_random_forest_test, truth=type_1, .pred_class) %>% 
  autoplot(type="heatmap")
```

### Exercise 10
The random forest did well. An ROC of 0.996 is great. I do notice a little hack that my random forest did. It appears that the random forest overpredicted the number of others. In fact, it appears that the algorithm did not predict the right specific class even close to our overall ROC level. Basically, when in doubt the algorithm assigned to other, which worked fairly well. I think if I was to run this again, I would upsample the number of the classes that are not other so that they are closer to equal levels to other. This would keep our algorithm from defaulting to other.

The algorithm is best at predicting other, worst at predicting bug. Other than the other class, the algorithm was the best at predicting psychic. As I know next to nothing about pokemon, I have no clue for why it was best at predicting this class rather than others.