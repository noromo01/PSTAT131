---
title: "classificationModels"
author: "Noah Moyer"
date: "2023-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Packages and read in dataset
```{r}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(here)
library(readxl)
library(corrr)
library(corrplot)
library(themis) #for step_upsample
tidymodels_prefer()
bikeClass <- read_excel(here("Project", "rawData", "bikingClass.xlsx"))
bikingOG <- bikeClass
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(janitor)
library(naniar)
library(xgboost)
library(ranger)
library(vip)
library(corrplot)
library(here)
library(ggplot2)
library(forcats)
```

### Split data

First, we need to split the data into our training and testing set
```{r}
#make the regressor variable (whether or not we are in the top 10) a factor
bikeClass$Rnk <- factor(bikeClass$Rnk)

#make the classification variables into factors
bikeClass$ParcourTypeCategorical <- factor(bikeClass$ParcourTypeCategorical)
bikeClass$WonByCategorical <- factor(bikeClass$WonByCategorical)

#remove unneccesary variable
bikeClass <- bikeClass %>% 
  select(-c(raceName, RiderName, RiderLastName, Team, WinningTimeHours, WinningTimeMinutes, Months, Days, VertMeters, StartlistQualScore))
```
I removed raceName,  RiderName, RiderLastName, and Team because all of these variables are identification variables and would not be useful in our analysis. I removed WinningTimeHours and WinningTimeMinutes because these are both variables that were used to determine WinningTimeMin ($Hours+Minutes*60$). I removed Months and Days because they were used to determine DaysIntoYear ($30.4*Months+Days$). I removed VertMeters because of the high correlation with ProfileScore. I know that ProfileScore was calculated using VertMeters. I removed StartlistQualScore because of the high correlation with RaceRanking. I know that StartlistQualScore was calculated using RaceRanking. I believe that other variables with high correlation, such as GCScore and Climber score as well as WinningTimeMin and Distance are still useful for our analysis. For example, in the rare case that a rider has a low GCScore but a high Climber score that tells us that the rider is very good at climbing but bad at other characteristics of a GC rider such as time trialing. Also, in the case that WinningTimeMin and Distance do not correlate as predicted this tells us that something must have happened in the race that caused it to be a lot slower. This could provide some interesting insights.

Below is the correlation plot before variables were removed for reference.
```{r}
#from EDA.rmd
cor_results <- bikingOG %>% 
  select(-Rnk) %>% 
  correlate()

cor_results %>% 
  stretch() %>% 
  ggplot(aes(x,y,fill=r)) +
  geom_tile() +
  geom_text(aes(label=as.character(fashion(r)))) +
  theme(axis.text.x=element_text(angle=-90))

```

We will proceed forward with splitting the data.

```{r}
set.seed(110)

#split the dataset by 0.75
bikeSplit <- initial_split(bikeClass, prop=0.75, strata="Rnk")

#set into training and test
bikeTrain <- training(bikeSplit)
bikeTest <- testing(bikeSplit)

#create a ten fold cross validation
bikeFold <- vfold_cv(bikeTrain, v=10, strata="Rnk")

```

### Building the recipe
Building the recipe for the bikeClass

```{r}
#build the recipe
bikeRecipe <- recipe(Rnk~., data=bikeTrain) %>% 
  step_dummy(all_nominal_predictors()) %>% #dummy code all the categorical variables
  step_normalize(all_predictors()) %>% #set all numeric variables such that they have a mean of 0 and variance of 1
  step_upsample(Rnk, over_ratio=0.5) #upsample Rnk (our response variable) such that top 10 is equal to a quarter of notTopTen
```

I am well aware that my response variable is very unbalanced. Thus, to make up for this I am upsampling. However, I don't want to overdo how much some variables may be copied so I only set the over_ratio to 0.5.

### Choosing models to fit to the data
I shall fit XYZ models to my classification dataset.
```{r}
#Logistic regression
log_reg <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm")

#Regularized regression
log_reg_reg <- logistic_reg(mixture=tune(),
                            penalty=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

#K-nearest neighbors
KNN <- nearest_neighbor(neighbors=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn")

#random forest
rand_for <- rand_forest(mtry=tune(),
                        trees=tune(),
                        min_n=tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

#boosted trees
boosted_for <- boost_tree(mtry=tune(),
                          trees=tune(),
                          learn_rate=tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```
Next, time to create some grids

```{r}
#logistic regularized regression grid
log_reg_reg_grid <- grid_regular(penalty(range=c(0,1),
                                 trans=identity_trans()),
                                 mixture(range=c(0,1)),
                                 levels=5)
#KNN grid
KNN_grid <- grid_regular(neighbors(range=c(2,8)),
                         levels=7)
#random forest grid
rand_for_grid <- grid_regular(mtry(range=c(1,8)),
                              trees(range=c(100,800)),
                              min_n(range=c(10,18)),
                              levels=8)

#boosted tree grid
boosted_for_grid <- grid_regular(mtry(range=c(1,8)),
                                 trees(range=c(100,800)),
                                 learn_rate(range=c(-10,-1)),
                                 levels=8)


```

Now we set up the workflows
```{r}
#workflow for the logistic regression model
log_reg_wkflw <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(bikeRecipe)

#workflow for the random forest
rand_for_wf <- workflow() %>% 
  add_model(rand_for) %>% 
  add_recipe(bikeRecipe)
```

Next, we fit the models
```{r}
#model fit for the logistic regression model
log_reg_fit <- fit_resamples(
  object=log_reg_wkflw,
  resamples=bikeFold
)

#model fit for the random forest
rand_for_fit <- tune_grid(
  object=rand_for_wf,
  resamples=bikeFold,
  grid=rand_for_grid,
  control=control_grid(verbose=TRUE)
)
```
Error: x Fold10: preprocessor 1/1, model 1/1 (predictions):
  Error in `predict_model()`:
  ! Some assessment set rows are not available at prediction time. ...