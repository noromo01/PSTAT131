---
title: "classificationModels"
author: "Noah Moyer"
date: "2023-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Packages and read in dataset
```{r}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(here)
library(readxl)
library(corrr)
library(corrplot)
library(themis) #for step_upsample
library(dplyr)
tidymodels_prefer()
bikeClass <- read_excel(here("Project", "rawData", "bikingClass.xlsx"))
bikingOG <- bikeClass #for the corr plot
```

### Split data

First, we need to split the data into our training and testing set
```{r}
#make the regressor variable (whether or not we are in the top 10) a factor
bikeClass$Rnk <- factor(bikeClass$Rnk)

#make the classification variables into factors
bikeClass$ParcourTypeCategorical <- factor(bikeClass$ParcourTypeCategorical)
bikeClass$WonByCategorical <- factor(bikeClass$WonByCategorical)

#remove unneccesary variable
bikeClass <- bikeClass %>% 
  select(-c(raceName, RiderName, RiderLastName, Team, WinningTimeHours, WinningTimeMinutes, Months, Days, VertMeters, StartlistQualScore))
```
I removed raceName,  RiderName, RiderLastName, and Team because all of these variables are identification variables and will not be useful in our analysis. I removed WinningTimeHours and WinningTimeMinutes because these are both variables that were used to determine WinningTimeMin: ($Hours+Minutes*60$). I removed Months and Days because they were used to determine DaysIntoYear: ($30.4*Months+Days$). I removed VertMeters because of the high correlation with ProfileScore. I know that ProfileScore was calculated using VertMeters. I removed StartlistQualScore because of the high correlation with RaceRanking. I know that StartlistQualScore was calculated using RaceRanking. I believe that other variables with high correlation, such as GCScore and Climber score as well as WinningTimeMin and Distance are still useful for our analysis. For example, in the rare case that a rider has a low GCScore but a high Climber score that tells us that the rider is very good at climbing but bad at other characteristics of a GC rider such as time trialing. Also, in the case that WinningTimeMin and Distance do not correlate as predicted this tells us that something must have happened in the race that caused it to be a lot slower. This could provide some interesting insights.

Below is the correlation plot before variables were removed for reference.
```{r}
#from EDA.rmd
cor_results <- bikingOG %>% 
  select(-Rnk) %>% 
  correlate()

cor_results %>% 
  stretch() %>% 
  ggplot(aes(x,y,fill=r)) +
  geom_tile() +
  geom_text(aes(label=as.character(fashion(r)))) +
  theme(axis.text.x=element_text(angle=-90))

```

We will proceed forward with splitting the data.

```{r}
set.seed(110)

#split the dataset by 0.75
bikeSplit <- initial_split(bikeClass, prop=0.75, strata="Rnk")

#set into training and test
bikeTrain <- training(bikeSplit)
bikeTest <- testing(bikeSplit)

#create a ten fold cross validation
bikeFold <- vfold_cv(bikeTrain, v=10, strata="Rnk")
```

### Building the recipe
Building the recipe for the bikeClass

```{r}
#build the recipe
bikeRecipe <- recipe(Rnk~., data=bikeTrain) %>% 
  step_dummy(all_nominal_predictors()) %>% #dummy code all the categorical variables
  step_normalize(all_predictors()) %>% #set all numeric variables such that they have a mean of 0 and variance of 1
  step_upsample(Rnk, over_ratio=0.5) #upsample Rnk (our response variable) such that top 10 is equal to a quarter of notTopTen
```

I am well aware that my response variable is very unbalanced. Thus, to make up for this I am upsampling. However, I don't want to overdo how much some variables may be copied so I only set the over_ratio to 0.5.

### Model choice and fitting
I shall fit 5 models to my classification dataset. I first decided to do a logistic regression as a good base model. The next model I decided to used was a regularized regression logistic model in order to return the best possible logistic regression using a combination of Lasso and Ridge regularization as well as tuning the penalty hyperparameter. I also decided to use a K-nearest neighbors (KNN) model as this model is distinctly different than the other models. I finally decided to use a random forest and a boosted tree model as these models are the most advanced and I expect them to return the best results.
```{r}
#Logistic regression
log_reg <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm")

#Regularized regression
log_reg_reg <- logistic_reg(mixture=tune(),
                            penalty=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

#K-nearest neighbors
KNN <- nearest_neighbor(neighbors=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn")

#random forest
rand_for <- rand_forest(mtry=tune(),
                        trees=tune(),
                        min_n=tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

#boosted trees
boosted_for <- boost_tree(mtry=tune(),
                          trees=tune(),
                          learn_rate=tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```

I am going to do some tuning in order to optimize by hyperparameters. For logistic regularized regression I chose 0 through 1 for penalty and mixture in order to get a wide range of penalties for the regularization and a wide range of mixtures between Lasso and Ridge. For KNN I chose one step increments between 2 and 12. I figure 1 is probably too specific but 2 through 12 seems to give a good range that will provide some useful insights. For my random forest, I chose the number of predictors to be randomly selected between each split to be between 1 and 16 because I have 26 predictors and want to get a good range of different values for how many predictors I have. I chose trees to be between 100 and 800 because this gives us lots of varying levels of forest size. For the minimum number of nodes I chose between 10 and 18 because this is a decent amount of nodes at each terminal branch but it is not too many. For the boosted tree, I used the same logic for the number of predictors selected at each split and trees as I used for the random forest. For learn rate I chose between -10 and -1 because this correlates to $1*10^{-10}$ through $0.1$ which gives us a wide variety of learning rates. I decided to drop the number of levels from 8 to 5 in order to decrease run time for the random forest and boosted forest. With 8 levels and three hyperparameters that we are trying to tune, this means that we would have $8^{3}=512$ models to run on each fold. By dropping to 5 we only have $5^{3}=125$ models to run which is a much more doable number.
```{r}
#logistic regularized regression grid
log_reg_reg_grid <- grid_regular(penalty(range=c(0,1),
                                 trans=identity_trans()),
                                 mixture(range=c(0,1)),
                                 levels=5)
#KNN grid
KNN_grid <- grid_regular(neighbors(range=c(2,12)),
                         levels=11)
#random forest grid
rand_for_grid <- grid_regular(mtry(range=c(8,20)),
                              trees(range=c(100,800)),
                              min_n(range=c(10,18)),
                              levels=5)

#boosted tree grid
boosted_for_grid <- grid_regular(mtry(range=c(8,20)),
                                 trees(range=c(100,800)),
                                 learn_rate(range=c(-10,-1)),
                                 levels=5)


```

Now we set up the workflows
```{r}
#workflow for the logistic regression model
log_reg_wkflw <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(bikeRecipe)

#workflow for the regularized regression
log_reg_reg_wkflw <- workflow() %>% 
  add_model(log_reg_reg) %>% 
  add_recipe(bikeRecipe)

#workflow for K-nearest neighbors
KNN_wkflw <- workflow() %>% 
  add_model(KNN) %>% 
  add_recipe(bikeRecipe)

#workflow for the random forest
rand_for_wkflw <- workflow() %>% 
  add_model(rand_for) %>% 
  add_recipe(bikeRecipe)

#workflow for the boosted tree
boosted_for_wkflw <- workflow() %>% 
  add_model(boosted_for) %>% 
  add_recipe(bikeRecipe)
```

Next, we fit the models. Note that this is going to take a while to run. Thus, I set eval to false so that I would not have to run this every time. I then saved the models at the end of this code so they could be loaded in later and analyzed without having to run this code every single time.
```{r, eval=FALSE}
#model fit for the logistic regression model
log_reg_fit <- fit_resamples(
  object=log_reg_wkflw,
  resamples=bikeFold
)
#save the model
save(log_reg_fit, file=here("Project", "modelRuns", "class_log_reg_fit.rda"))

#model fit for regularized regression
log_reg_reg_fit <- tune_grid(
  object=log_reg_reg_wkflw,
  resamples=bikeFold,
  grid=log_reg_reg_grid,
  control=control_grid(verbose=TRUE)
)
save(log_reg_reg_fit, file=here("Project", "modelRuns", "class_log_reg_reg_fit.rda"))

#model fit for k-nearest-neighbors
KNN_fit <- tune_grid(
  object=KNN_wkflw,
  resamples=bikeFold,
  grid=KNN_grid,
  control=control_grid(verbose=TRUE)
)
save(KNN_fit, file=here("Project", "modelRuns", "class_KNN_fit.rda"))

#model fit for the random forest
rand_for_fit <- tune_grid(
  object=rand_for_wkflw,
  resamples=bikeFold,
  grid=rand_for_grid,
  control=control_grid(verbose=TRUE)
)
save(rand_for_fit, file=here("Project", "modelRuns", "class_rand_for_fit.rda"))

#model fit for boosted tree
boosted_for_fit <- tune_grid(
  object=boosted_for_wkflw,
  resamples=bikeFold,
  grid=boosted_for_grid,
  control=control_grid(verbose=TRUE)
)
save(boosted_for_fit, file=here("Project", "modelRuns", "class_boosted_for_fit.rda"))
```
