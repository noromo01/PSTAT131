---
title: "results"
author: "Noah Moyer"
date: "2023-03-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Read in packages and models
```{r}
library(here)
library(tidyverse)
library(tidymodels)
library(vip) #for the variable importance plot
library(ggplot2)
library(readxl) #for the MSR2023 excel sheet

#read in the model runs from the classification code, rename to names that make sense
load(here("Project", "modelRuns", "class_boosted_for_fit.rda"))
class_boosted_for <- boosted_for_fit

load(here("Project", "modelRuns", "class_KNN_fit.rda"))
class_KNN <- KNN_fit

load(here("Project", "modelRuns", "class_log_reg_fit.rda"))
class_log_reg <- log_reg_fit

load(here("Project", "modelRuns", "class_log_reg_reg_fit.rda"))
class_log_reg_reg <- log_reg_reg_fit

load(here("Project", "modelRuns", "class_rand_for_fit.rda"))
class_rand_for <- rand_for_fit

#read in the model runs from the regression code, rename to names that make sense
load(here("Project", "modelRuns", "reg_boosted_for_fit.rda"))
reg_boosted_for <- boosted_for_fit

load(here("Project", "modelRuns", "reg_KNN_fit.rda"))
reg_KNN <- KNN_fit

load(here("Project", "modelRuns", "reg_lin_reg_fit.rda"))
reg_lin_reg <- lin_reg_fit

load(here("Project", "modelRuns", "reg_lin_reg_reg_fit.rda"))
reg_lin_reg_reg <- lin_reg_reg_fit

load(here("Project", "modelRuns", "reg_rand_for_fit.rda"))
reg_rand_for <- rand_for_fit
```

```{r, include=FALSE}
#run some old code so we can use the workflow
#class
bikeClass <- read_excel(here("Project", "rawData", "bikingClass.xlsx"))
bikeClass$Rnk <- factor(bikeClass$Rnk)
bikeClass$ParcourTypeCategorical <- factor(bikeClass$ParcourTypeCategorical)
bikeClass$WonByCategorical <- factor(bikeClass$WonByCategorical)
bikeClass <- bikeClass %>% 
  select(-c(raceName, RiderName, RiderLastName, Team, WinningTimeHours, WinningTimeMinutes, Months, Days, VertMeters, StartlistQualScore))
set.seed(110)
bikeSplitClass <- initial_split(bikeClass, prop=0.75, strata="Rnk")
bikeTrainClass <- training(bikeSplitClass)
bikeTestClass <- testing(bikeSplitClass)
bikeRecipeClass <- recipe(Rnk~., data=bikeTrainClass) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_upsample(Rnk, over_ratio=0.5)
boosted_for_class <- boost_tree(mtry=tune(),
                          trees=tune(),
                          learn_rate=tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
boosted_for_wkflw_class <- workflow() %>% 
  add_model(boosted_for_class) %>% 
  add_recipe(bikeRecipeClass)
#reg
bikeReg <- read_excel(here("Project", "rawData", "bikingDF.xlsx"))
bikeReg$ParcourTypeCategorical <- factor(bikeReg$ParcourTypeCategorical)
bikeReg$WonByCategorical <- factor(bikeReg$WonByCategorical)
bikeReg <- bikeReg %>% 
  select(-c(raceName, RiderName, RiderLastName, Team, WinningTimeHours, WinningTimeMinutes, Months, Days, VertMeters, StartlistQualScore))
set.seed(110)
bikeSplitReg <- initial_split(bikeReg, prop=0.75, strata="Rnk")
bikeTrainReg <- training(bikeSplitReg)
bikeTestReg <- testing(bikeSplitReg)
bikeRecipeReg <- recipe(Rnk~., data=bikeTrainReg) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors())
boosted_for_reg <- boost_tree(mtry=tune(),
                          trees=tune(),
                          learn_rate=tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")
boosted_for_wkflw_reg <- workflow() %>% 
  add_model(boosted_for_reg) %>% 
  add_recipe(bikeRecipeReg)
```

### Analyzing how the classification models did
First, let's see how the different models that we tuned performed across their tuning parameters
```{r}
#use autoplot to see how the tuned models performed
autoplot(class_KNN)
autoplot(class_log_reg_reg)
autoplot(class_rand_for)
autoplot(class_boosted_for)

```

Note that there is no plot of how the tuning did for the logistic regression did as we did not tune the logistic regression.
Starting with K-nearest neighbors it appears that more neighbors lead to a better "area under the curve" (AUC). We should note, however, that accuracy drops between five and six neighbors and again between 10 and 11. It does not drop by a whole lot so I am not too worried. It appears to start leveling out around 11 or 12 so I believe that this number of neighbors may be the best decision.
The regularized logistic regression provides some interesting insights. The best amount of penalty appears to be a low penalty. However, if we have no regularization, a complete ridge regularization, penalty does not matter and AUC appears to be just about the highest.
The random forest graph has a lot going on. The most notable trend appears to be that as we increase the number of randomly selected predictors, AUC falls. Thus, having a low number of randomly selected predictors appears to be best. Also, it appears as if generally a higher number of trees is better but as long as you have more than 100 trees, the AUC does not vary a whole lot. Minimum node size does not appear to be super important on AUC.
The boosted forest plots look a lot different than the random forest which is interesting. There is not as much variation across plots, they are mostly straight lines. Thus, for a boosted forest the number of randomly selected predictors does not appear to have a large influence on model performance. However, learning rate does. It appears that the best learning rate is the highest at 0.1. Also, other than the lowest learning rate, the number of trees in the forest does not matter as well. The only thing that appears to matter is the learning rate.

Let's now see for each model set up, which specifically tuned model performed the best.
```{r}
#show the best 10 performing models for each model
collect_metrics(class_log_reg) #no top 10 because no tuning was conducted
show_best(class_KNN, n=10, metric='roc_auc')
show_best(class_log_reg_reg, n=10, metric='roc_auc')
show_best(class_rand_for, n=10, metric='roc_auc')
show_best(class_boosted_for, n=10, metric='roc_auc')
```
Comparing the best model across models it appears that the K-nearest neighbors performed the worst. Interestingly enough, the regularized logistic regression performed worse than the non-regularized version. The two forest models performed the best with the boosted forest slightly outperforming the random forest. Thus, the best model appears to be the boosted forest with 17 randomly selected predictors, 450 trees, and a learning rate of 0.1.
Let's test this model on the testing set!
```{r}
#select the best boosted forest
best_class_mod_class <- select_best(class_boosted_for, metric='roc_auc')

#fit to training set
final_boosted_class <- finalize_workflow(boosted_for_wkflw, best_class_mod_class) %>% 
  fit(bikeTrain)

#evaluate on the testing set
best_class_testing <- augment(final_boosted_class, bikeTest) %>% 
  select(Rnk, starts_with(".pred"))

#evaluate the model
roc_auc(best_class_testing, truth=Rnk, .pred_notTop10)
```

The model worked even better than on our training set! Let's see the variable importance plot.

```{r}
#variable importance plot
final_boosted_class %>%
  extract_fit_parsnip() %>% 
  vip(num_features=26)
```
This variable importance plot is super interesting. It makes sense that PCSRanking would be head and shoulders the best predictor of race results. This is the overall ranking of how good a rider is so obviously this would be a good predictor of rider performance. It makes sense that some of the next best predictors are the various components that say how good a rider is. It is interesting to see and makes me feel good about my model set up that the important variables are a mix from the rider profile dataset and the race profile dataset. It shows that the race set up and the individual rider play a major roll in how the race is won (or how riders get in the top 10). I am also happy to see that while there are definitely certain variables that are more important to others, all variables had some role to play
Let's look at the ROC curve.
```{r}
#plot the ROC curve.
roc_curve(best_class_testing, truth=Rnk, .pred_notTop10) %>% 
  autoplot()

```
The ROC curve looks about as we would expect. It is by no means perfect but it does a lot better than just random guessing.
Let's create a confusion matrix next.
```{r}
#confusion matrix
conf_mat(best_class_testing, truth=Rnk, .pred_class) %>% 
  autoplot(type="heatmap")

```

The confusion matrix reveals a lot about this model. It looks like the model got very good at predicting when a rider would not finish in the top 10 and was very hesitant to guess that a rider would finish in the top 10. The model did have a pretty low error rate when it comes to predicting a rider to finish in the top 10 given that the rider did finish in the top 10 (note that when I say "pretty low" I mean that it has a below 50% error rate, which is not great). This definitely reveales that the model is not doing as well as the ROC says it was doing.

Now to see how the regression models did.

### Analyzing how the regression models did

First, let's see how the different models that we tuned performed across their tuning parameters
```{r}
#use autoplot to see how the tuned models performed
autoplot(reg_KNN)
autoplot(reg_lin_reg_reg)
autoplot(reg_rand_for)
autoplot(reg_boosted_for)

```
My initial reaction is that throughout we have a very low r-squared value. It appears that none of our models did a very good job at predicting race outcome.
For k-nearest neighbors the models follow the same trend that the KNN model did for the classification models, the more neighbors the better the model performed.
For our regularized linear regression, we see another similar trend. Zero regularization, a complete ridge regression, seems to perform best across penalties. As regularization increases, the best models are those with a low penalty.
Our random forest looks considerably different. First off, the number of trees and the number of nodes at a terminal branch does not appear to affect our r-squared value. The number of predictors also does not vary a whole lot as long as you do not pick two. Number of predictors greater than two works almost equally well across models.
Our boosted forest shows some interesting results as well. The best r-squared values occur when our learning rate is 0.1. With a learning rate of 0.1, the models with more trees appeared to perform better. We also see a similar trend to the trend in the random forest with the number of randomly selected predictors. As long as the number of predictors is greater than two, they all performed somewhat equally well.

Let's now see for each model set up, which specifically tuned model performed the best.
```{r}
#show the best 10 performing models for each model
collect_metrics(reg_lin_reg) #no top 10 because no tuning was conducted
show_best(reg_KNN, n=10, metric='rsq')
show_best(reg_lin_reg_reg, n=10, metric='rsq')
show_best(reg_rand_for, n=10, metric='rsq')
show_best(reg_boosted_for, n=10, metric='rsq')
```
Unlike the classification models, the K-nearest neighbors model outperformed the regularized and unregularized linear regression. The best regularized linear regression slightly outperformed the un-tuned linear regression. Similar to the classification models, the best models were the random forest models. It is also notable just by how much the random forest models out performed the other models. Once again, the boosted forest slightly outperformed the normal random forest. Thus, we will use this model in the rest of our analysis.

Let's test this model on the testing set!
```{r}
#select the best boosted forest
best_class_mod_reg <- select_best(reg_boosted_for, metric='rsq')

#fit to training set
final_boosted_reg <- finalize_workflow(boosted_for_wkflw_reg, best_class_mod_reg) %>% 
  fit(bikeTrainReg)

#evaluate on the testing set
best_reg_testing <- augment(final_boosted_reg, bikeTestReg) %>% 
  select(Rnk, starts_with(".pred"))

#evaluate the model
rsq(best_reg_testing, truth=Rnk, estimate=.pred)
```

The model worked about the same as our testing set. While it did not perform the best, we were asking the model to do a lot so I am not super dissapointed in this result.

```{r}
#variable importance plot
final_boosted_reg %>%
  extract_fit_parsnip() %>% 
  vip(num_features=26)
```
I am interested to see that the variable importance plot looks somewhat different than the variable importance plot from the classification data. We have the same top performers but in addition to the drop from PCSRanking to ClimberScore there is another drop from GCScore to NumFinished. This means that the model was using GCScore and ClimberScore more than just PCSRanking. In fact, PCSRanking dropped from having an importance of around 0.27 to having an importance of 0.17. 

Let's create a predicted vs actual value plot
```{r}
best_reg_testing %>% 
  ggplot(aes(x=.pred, y=Rnk)) +
  geom_point(alpha=0.4, size=0.5) +
  geom_abline(lty=2) +
  labs(title="Predicted and Actual Values of Rank", x="Prediction", y="Actual Value")

```
Overall, it looks like the model performed fairly well. It is definitely worth noting that the algorithm does not seem to understand the concept of ranges. For example, it is impossible for you to finish in a place that is negative, which the model did predict a handful of times. All in all, the model did a pretty good job of getting a general range of where people would finish. I think it is worth noting that it appears as if there are more values below the line at low values and more values above the line at high values. I think this is because the model was not great at predicting surprise winners or people who surprised and did well. The model also was not good at predicting when a rider who should have done better than they did disappointing. The model has no way of knowing why they disappointing. For example, it could have been because the favorite crashed which the model would have no way of knowing.

### Predicting MSR
Let's see how the boosted tree regression model does at predicting Milano Sanremo, one of the first major races of the 2023 calendar that happened on Saturday, March 18th. I only included the rider profile of the favorites for the race.
```{r}
#read in the dataset
MSR2023 <- read_excel(here("Project", "rawData", "MSR2023.xlsx"))
MSR2023conv <- MSR2023

#remove unnecessary variables and factorize chategorical variables
MSR2023$ParcourTypeCategorical <- factor(MSR2023$ParcourTypeCategorical)
MSR2023$WonByCategorical <- factor(MSR2023$WonByCategorical)
MSR2023 <- MSR2023 %>% 
  select(-c(raceName, RiderLastName, Team, WinningTimeHours, WinningTimeMinutes, Months, Days, VertMeters, StartlistQualScore))

#evaluate on the boosted tree model
MSRPred <- augment(final_boosted_reg, MSR2023) %>% 
  select(Rnk, starts_with(".pred"), RiderName)

#Let's see how the model did
head(MSRPred)
```
I don't think the model did that bad! It obviously is not great that it is giving Wout Van Aert a negative race result but putting Van Aert as the favorite for the race is not a bad call, he won it in 2020. Filippo Gana getting such a high prediction finishing result is not unreasonable. While people definitely had him down as an outsider favorite, he has not previously performed at this level in this type of race. A factor that made him a heavier favorite is the fact that his teammate, Tom Pidcock, was ruled out of the race last minute elevating him to sole leader on the team. This means that he had the full resources of the team behind him to push for the win. This factor was not included in our model. Mathieu being third favorite out of this list and then winning is a very reasonable reasonable. He hasn't had any standout performances for the road biking season yet so it was somewhat surprising to see him in such good form and winning the race.

### Conclusion
Overall, I would say that the models performed better than expected. There are a lot of variables that go into a bike race and the models were able to do the best they could. There were a lot of important aspects to a bike race that was not able to be contained in this model. Some of the factors missing include whether the rider crashed, how the rider woke up feeling, whether the rider was in their top shape, and many more.
Overall, the boosted tree model performed the best across both models. This probably should not be super surprising as the boosted tree model was the most advanced model that we used.
I believe that the regression model worked better than the classification model. Because of how the classification model was split, the model worked to be really good at predicting when someone would not finish in the top ten instead of trying to figure out when someone would actually finish in the top ten. With a startlist of normally close to 200 riders per race and just a handful of favorites, it is not very profound to predict that a certain rider would not finish in the top ten. We want to know who is going to be in the top ten! If I were to run the classification problem again, I would at the very least change the over_ratio to a value closer to 1. I also would consider creating multiple classifications. This way the model would focus on more than just the riders in the top 10. While the regression model was not perfect, I do believe it worked better. It also provides some power in that if you are trying to predict which out of two riders will perform better, a numeric value that will be larger or smaller than another numeric value provides insights. This is better than the classification model where two favorites may just produce the result of "top10" for both riders.
For both models, the most important variable by far was PCSRanking. This is probably because PCSRanking is a function of how well a rider does throughout the year. This reveals a potential weakness in our model. Because PCSRanking is a function of results, the results that we are using as the response variable will be a factor in PCSRanking. This is probably why PCSRanking was the most important variable. It is important to note that PCSRanking is determined by how well the rider did throughout the year, not solely how well a rider did in one race. If I were to run the models again, I would consider removing the PCSRanking variable and other variables that are a fucntion of how well the rider has done in races that are the response variable. Another way to do this would be to imput PCSRanking as a value before the race result was known. This would required somewhat complicated coding but would provide better insights.
I believe that by the fact that there are more important variables in the response boosted model than in the classification boosted model reveals that the response model was using more variables to make its conclusion. This is a good thing because it shows that the response model was thinking about things beyond a riders ranking in order to make its decision on rank.
I would be interested to analyze how the model performance would change throughout years. If I used this model to predict riders racing this year, would it do worse than last year? I think it would because the model that I built was based on racing in 2022. The racing strategy will probably have changed between 2022 and 2023, if ever so slightly. If we compounded this out to ten years then I think this effect would be more pronounced.
If I were to build this model again, I would like to make sure the variables I use are not dependent on the response variable. I would also be interested to try to add variables such as "number of crashes" or "number of mechanicals". I think this could provide insight into the race. I also think it would be interesting to introduce a variable that says something about momentum. A variable that has a value that shows how a rider has done in the past 10ish races would be cool because then riders who are on a winning streak could be predicted to keep winning by the model.
Overall, this was a big problem for a machine learning algorithm to solve. I think it did the best it could with the information given but a lot of information is very difficult to quantify and thus would be difficult for a machine learning algorithm to use.